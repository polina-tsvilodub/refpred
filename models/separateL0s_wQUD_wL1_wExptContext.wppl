// ///fold:


// // helper function
var exp = function(x){return Math.exp(x)}

// // helper function
var marginalize = function(dist, key){
  return Infer({model: function(){sample(dist)[key]}})
}

// // for discretization

var binParam = 5;

var round = function(x){
  return Math.round(x*10)/10
}

// introduce parameters for three possible subordinate and a basic category
var stateParams = {
  dog1: {mu: 1, sigma: 0.5},
  dog2: {mu: 0, sigma: 0.5},
  dog3: {mu: -1, sigma: 0.5},
  super: {mu: 0, sigma: 1}
};

// rangle of possible sizes
var stateVals = map(
  round,
  _.range(stateParams.super.mu - 2 * stateParams.super.sigma,
          stateParams.super.mu + 2 * stateParams.super.sigma + stateParams.super.sigma/binParam,
          stateParams.super.sigma/binParam)
);

// probabilitites of possible sizes depending on cc
var stateProbs = cache( function(cc) {
  return map(function(s){
    Math.exp(Gaussian(stateParams[cc]).score(s))+
    Number.EPSILON
  }, stateVals)
});


/////////////////////
// inspired by dynamic semantics & discourse representation theory
// the idea is the following:
// main points made in E3 and hence to be made in the model in general:
// 1. main effect of context
// 2. (main) effect of basic vs subordinate noun choice
// 3. NP X syntax interaction in basic-context

// This means we need a context representation, which can change independently of
// (/it is a different thing than) the world knowledge, represented by the statePrior

// dynamic semantics provides a notion of meaning as the change of knowledge state of receiver
// upon interpreting the message -- for our purposes we could view it as e.g. reduction in
// uncertainty about the intended referent

// discourse representation theory provides the framework, in the sense that there is
// some common discourse represented by the context, and then the referent (and its property)
// are accomodated upon receiving the utterance

// by this we hope to introduce context-sensitivity of the different utterances
// and to solve the problem of literal semantics of 'that'

//////////////////////////
// most important new components of this model version:
// the contexts: explicit contexts corresponding to the ones in E3
// a boolean representing whether reference is established or not
// allowing utterances with one N only
// include 'world knowledge' (i e statPrior) for 3 different sub categories
// there are 2 distinct L0s for the 2 QUDs
/////////////////////////////
// generate the uniform threshold prior
var thresholdBins = function(form, stateSupport){
  return map(function(x){
    return form == "positive" ? x - (1/(binParam*2)) : x + (1/(binParam*2));
  }, sort(stateSupport))
}

var thresholdPrior = function(form, stateSupport){
  return Infer({
    model: function() { return uniformDraw(thresholdBins(form, stateSupport)) }
  });
};

var statePrior = function(cc) {
  Infer({
//    method: 'MCMC',
    model: function() {
      return categorical({vs: stateVals, ps: stateProbs(cc)})
//     return {
//       referent: categorical({vs:["dog1", "dog2", "dog3"], ps: [1,1,1]}),
//       property: categorical({vs: stateVals, ps: stateProbs(cc)})
//     }
   }
  })
};


var context = {
  // basic-level context, with a 2 members from three different subordinate categories
  super: [ {referent: "dog1", property: 1, name: "A"},
         {referent: "dog1", property: 1.6, name: "B"},
         {referent: "dog2", property: 0, name: "C"},
         {referent: "dog2", property: 0.2, name: "D"},
         {referent: "dog3", property: -1, name: "E"},
         {referent: "dog3", property: -1, name: "F"}
  ],
  sub: [ {referent: "dog1", property: 1, name: "A"},
         {referent: "dog1", property: 1, name: "B"},
         {referent: "dog1", property: 1, name: "C"},
         {referent: "dog1", property: 1, name: "D"},
         {referent: "dog1", property: 1, name: "E"},
         {referent: "dog1", property: 1, name: "F"}
  ]
}
// possible statePrior parameters are adjusted to represent three different subordinate categories (above)

// additionally to the context representation, there is a variable whether the target referent
// is already accomodated or not, i.e. if reference is established or not

/////////////////////////////
// functions for the L1 later on

// the L1 is going to infer if reference is established or not;
// however this might be not entirely plausible given our experimental setup

// S1 knows its value, L0 gets it
var establishRef = Infer({
  model: function(){return uniformDraw([true, false])}
});

var qudPrior = Infer({
  model: function(){return uniformDraw(["ref", "pred"])}
});

// referent prior
var targetPrior = function(context){ // gets the respective list, for L1
  Infer({
    model: function(){return uniformDraw(context)}
  });
}

// assume a uniform prior over comparison classes
var classPrior = Infer({
  model: function(){return uniformDraw(["sub", "super"])}
});

/////////////////////////


// prior N position distribution (uniform)
var Nposition = function(qud) {
  Infer({ // think if Infer is necessary here
    model: function(){
      //return uniformDraw(['subj', 'pred'])
      var syntax = qud == "ref" ? flip(0.8) ? "subj" : "pred" : flip(0.8) ? "pred" : "subj"
      return syntax
    }
  })
}



// now meaning of the referential utteraqnce depends on establishRef:
// if T: state matches the target that L0 gets from S1.
// if F: state (i e the referent) needs to be sampled from the context
// this allows to have a generic meaning representation of all the utterances,
// and their utility depends on the state space restriction provided by establishRef
// "that", "dog" true of any referent, "sub" true if matching the category

// there are two meaning functions, for each L0 respectively
var meaningContext = function(utterance, state, establishRef) { // assume utterance is subject only
  var truthVal = utterance == 'that' ? true : utterance == "dog" ? true : utterance == state.referent ? flip(0.999) : flip(0.001)
  return truthVal // still can't get rid of the flips because of the sub context condition
} // I cannot get rid of the flips because of the conditions when ref = T and hence state = target

// var meaningContext = function(utterance, state, establishRef) { // assume utterance is subject only
//   var truthVal = utterance == 'that' ? true : utterance == "dog" ? true : establishRef == true? true: utterance == state.referent
//   return truthVal
// }

// standard adjective meaning
var meaningPred = function(utterance, threshold, state){ // assume utterance is adjective only
  var truthVal = utterance == "big" ? state.property > threshold ? flip(0.999) : flip(0.001):
                                      state.property < threshold ? flip(0.999) : flip(0.001)
  return truthVal
}

// only 1 N can occur in the utterance, its position Npos is randomly sampled by S1
var utterance = function(form, Npos) {
  var subject = Npos == "subj" ? uniformDraw(["dog1", "dog2", "dog3",  "dog"]) : "that";
  var predicate = Npos == "pred" ? form == "positive" ? uniformDraw(["big dog1", "big dog2", "big dog3", "big dog"]) :
                                       uniformDraw(["small dog1", "small dog2", "small dog3", "small dog"]):
                                  form == "positive" ? "big" : "small";
  return {subject, predicate}
}

// "dog2", "dog3",
// L0 accomplishing reference
var literalListenerRefInContext = function(utterance, context, target){
  Infer({
    model: function(){
      // under this representation, the context and reference influence
      // the representation of the (possible) states:
      // having established a referent constrains the set of possible states,
      // whereas no reference is formalised via the context which provides the possible states
      // (constraining the general world knowledge to specific individuals)
//       var state =  uniformDraw(context); //establishRef == true ? target :
//       var state = establishRef == true ? target : uniformDraw(context);
//       print(target)
      var state = uniformDraw(_.union(context, [target]));
//       print(_.union(context, [target]))
      // get the meaning
      var m = meaningContext(utterance, state)
      // the context representation includes names of the single individuals
      // otherwise, the given basic level context is treated as consisting of 3 individuals
      // having the individual names doe not really influence qualitative behavior,
      // but makes L0s preformance kind of 'conceptually complete'
//       establishRef == true ? (condition(m) && condition(state === target)) : condition(m)
//       var cond = establishRef == true ? (m && (state.referent == target.referent)) : m
      var cond = m && (state.referent == target.referent)
      //       print(cond)
//       condition(cond)
      condition(m)
      return {referent: state.referent }//, name: state.name}
    }
  })
}
// viz(literalListenerRefInContext('dog', context['sub'], false, {referent: 'dog1', property: 0.6}))
// viz(literalListenerRefInContext('dog1', context['sub'], false, {referent: 'dog1', property: 0.6}))
// viz(literalListenerRefInContext('that', context['sub'], false, {referent: 'dog1', property: 0.6}))
// viz(literalListenerRefInContext('dog2', context['sub'], false, {referent: 'dog1', property: 0.6}))
// viz(literalListenerRefInContext('dog3', context['sub'], false, {referent: 'dog1', property: 0.6}))
// viz(literalListenerRefInContext('that', context['super'], false, {referent: 'dog1', property: 0.6}))

// viz(literalListenerRefInContext('dog', context['super'], true, {referent: 'dog2', property: 0.6}))
// viz(literalListenerRefInContext('dog2', context['super'], true, {referent: 'dog2', property: 0.6}))
// viz(literalListenerRefInContext('that', context['super'], true, {referent: 'dog2', property: 0.6}))
// viz(literalListenerRefInContext('dog', context['sub'], true, {referent: 'dog2', property: 0.6}))
// viz(literalListenerRefInContext('dog2', context['sub'], true, {referent: 'dog2', property: 0.6}))
// viz(literalListenerRefInContext('that', context['sub'], true, {referent: 'dog2', property: 0.6}))


// L0 accomplishing predication
// NB: I could also put the two L0s into one and add a QUD variable (like in the previous model version),
// but if separating them is legitimate this way it's clearer and the meaning function doesn't
// depend on the QUD (MH's comment)

var literalListenerPredInContext = function(utterance, context, target, cc, threshold, subordinate){
  Infer({
    model: function(){
  // as before, the CC influences sampling the state from prior knowledge
      // i think this assumption is reasonable as long as we stipulate conceptual comparison classes,
      // (not sth like 'big dog' said of a pug surrounded by toy dogs) which we indeed do experimentally
      var splitPred = utterance.split(" ");
      var explicitCC = splitPred.length == 1 ? cc : splitPred[1];
   // get the category of the CC
      var cc = explicitCC == "dog" ?  "super" :
               explicitCC == "sub" ? subordinate : explicitCC
//       print(cc)
      var state = {
        property: sample(statePrior(cc))
        }
//       print(state)
      var m = meaningPred(splitPred[0], threshold, state)
//       print(m)
      condition(m)

//       return {property: target.property, c: cc}//
      return {property: state.property, c: cc}
      }
  })
}
// display("predication")
// viz.marginals(literalListenerPredInContext('big', context['super'], {referent: 'dog1', property: 0.6},"super", 0, "dog1"))
// viz(literalListenerPredInContext('big dog', context['super'], false, {referent: 'dog1', property: 0.6}, "super", 0))
// viz(literalListenerPredInContext('big dog', context['super'], false, {referent: 'dog1', property: 0.6}, "low", 0))
// viz(literalListenerPredInContext('big dog1', context['super'], false, {referent: 'dog1', property: 0.6}, "super", 0))

// the speaker has a state in mind, a context, knows whether reference was established,
// knows the form of the adjective he wants to use, a threshold and has a CC in mind (for subject-N cases)
var speakerContext = function(state, context, qud, form, threshold, cc, subordinate){
  Infer({
    model: function() {
      var Npos = sample(Nposition(qud)) // sample the position of the N
      var utt = utterance(form, Npos) // sample an utterance
      var subject = utt.subject
//       print(utt)
      var predicate = utt.predicate
      // get reference
      var L0ref = literalListenerRefInContext(subject, context, state)
//       print("L0ref")
//       print(L0ref)
      // get predication
      var L0pred = literalListenerPredInContext(predicate, context, state, cc, threshold, subordinate)
//       print("L0pred")
//       print(L0pred)
//       compute the two utilities
      var cc2assign = cc == 'sub' ? subordinate : "super";
      // again, I assume subject accomplishes reference, predicate accomplishes predication
      var refUt = L0ref.score({referent: state.referent})//, name: state.name})
      var predUt = L0pred.score({property: state.property, c: cc2assign})
      // maximize sum utility (alpha randomly set to 3)
      var cond = qud == "ref" ? refUt : predUt
//       factor(3*(refUt + predUt))
      factor(3*cond)
      return utt
    }
  })
}

// the model makes nice predictions:
// when reference = T, it prefers a N in predicate position and
// disprefers the sub N with decreasing size of the referent (plot 1 vs plot 5)
// when reference = F, it preferes a sub N in subject position given basic level context (e g plot 2 vs plot 1)
// however, as predicted in sub context there is no difference in subj N preference when reference varies:
// this is because all referring expression have equal utility in sub context

// display("Basic context, big Great Dane, reference = T")
// viz(speakerContext({referent: 'dog1', property: 1, name:"A"}, context['sub'], "pred", "positive", -2.1, "sub", "dog1") )
// viz(speakerContext({referent: 'dog1', property: 1, name:"A"}, context['super'], "pred", "positive", -2.1, "sub", "dog1") )
// viz(speakerContext({referent: 'dog3', property: -1, name:"E"}, context['super'], "pred", "negative", 0, "sub", "dog3") )
// viz(speakerContext({referent: 'dog2', property: 1, name:"C"}, context['super'], "pred", "positive", 0, "super", "dog2") )


// for a more plausible implementation of our experimental set-up, the L1 knows
// the subordinate category of the referent and its 'name' (let's see if we actually need it)
// what is unknown, is its precise size -- sampled from a state prior over the
// corresponding subordinate distribution (as in original CC model)
// referent = {sub: , name: 'X'}
var pragmaticListener = function(utterance, form, cont, referent) {
  Infer({model: function(){
    // uncertainty about the comparison class (super vs. sub)
//     var c = sample(classPrior)
    // add the context as potential comparison class
    var c = uniformDraw(['sub', 'super', cont])
//     print(c)
    var currentContext = context[cont]
//     var target = sample(targetPrior(currentContext))
    var subordinate = referent.sub;
    // L1 doesn't know the exact size of the referent
    var size = sample(statePrior(subordinate))
//     print(size)
    var target = {referent: referent.sub, property: size, name: referent.name}
    // var subordinate = target.referent
//     print(target)
//     var isReference = sample(establishRef)
    var qud = sample(qudPrior)
//     print(isReference)
    var currentStatePrior = statePrior(subordinate)
    var threshold = sample(thresholdPrior(form, currentStatePrior.support() ))
//     print(threshold)
//     print(state)
    var S1 = speakerContext(target, currentContext, qud, form, threshold, c, subordinate);
//     print(S1)
    observe(S1, utterance);
    return { comparisonClass: c, state: size, QUD: qud}
  }})
}
viz.marginals(pragmaticListener({subject: "dog1", predicate: "big"}, "positive", "super", {sub: "dog1", name: "C"}))
viz.marginals(pragmaticListener({subject: "that", predicate: "big dog1"}, "positive", "super", {sub: "dog1", name: "C"}))
// display("small")
// viz.marginals(pragmaticListener({subject: "dog3", predicate: "small"}, "negative", "super", {sub: "dog3", name: "C"}))
// viz.marginals(pragmaticListener({subject: "that", predicate: "small dog3"}, "negative", "super", {sub: "dog3", name: "C"}))


display("sub")
viz.marginals(pragmaticListener({subject: "dog1", predicate: "big"}, "positive", "sub", {sub: "dog1", name: "C"}))
viz.marginals(pragmaticListener({subject: "that", predicate: "big dog1"}, "positive", "sub", {sub: "dog1", name: "C"}))


// viz.marginals(pragmaticListener({subject: "that dog1", predicate: "big"}, "positive", "super"))
// var currentStatePrior = statePrior("high")
// sample(thresholdPrior("positive", currentStatePrior.support() ))
// // currentStatePrior
// literalListenerRefInContext("dog3", context["super"], true, {"referent":"dog1","property":1,"name":"A"})
